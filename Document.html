<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://vuepress-theme-hope-docs-demo.netlify.app/AIGCDetect/Document.html"><meta property="og:title" content="Experiment Document"><meta property="og:description" content=".mytable2 { font-size: 14px; font-family: Arial, Helvetica, sans-serif; width: 100% !important; border-collapse: collapse; align-items: center; / margin-top: 20px !important; / ..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="article:author" content="Yiran Xu"><meta property="article:published_time" content="2023-09-22T00:00:00.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Experiment Document","image":[""],"datePublished":"2023-09-22T00:00:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"Yiran Xu","url":"https://aemilia-xu.github.io/"}]}</script><title>Experiment Document</title><meta name="description" content=".mytable2 { font-size: 14px; font-family: Arial, Helvetica, sans-serif; width: 100% !important; border-collapse: collapse; align-items: center; / margin-top: 20px !important; / ...">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/AIGCDetect/assets/style-585e6a11.css" as="style"><link rel="stylesheet" href="/AIGCDetect/assets/style-585e6a11.css">
    <link rel="modulepreload" href="/AIGCDetect/assets/app-d6f521c8.js"><link rel="modulepreload" href="/AIGCDetect/assets/Document.html-37ac66ab.js"><link rel="modulepreload" href="/AIGCDetect/assets/Document.html-0e0d17c8.js"><link rel="prefetch" href="/AIGCDetect/assets/Awesome-AIGCDetection.html-b149031a.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-2074a3ea.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/slides.html-24820c7d.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-35c8edb6.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-0f4bd41e.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-dd2c76eb.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-d3fe78a7.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/slides.html-336468c2.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/baz.html-9803ee47.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-025cdba0.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/ray.html-b859e0da.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-46ad987c.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/disable.html-66c07bce.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/encrypt.html-6b902909.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/markdown.html-92c03f40.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/page.html-18e02e3d.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-54d33b96.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-c555f825.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/baz.html-89399bd7.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-b71f2403.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/ray.html-4768a723.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-7e557c43.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/404.html-8df2da22.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/Awesome-AIGCDetection.html-be2cf7e0.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-fa0febb8.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/slides.html-99486d3d.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-fc2abe49.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-dec5d210.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-ee245fe1.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-3b8e7446.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/slides.html-4f22fd5b.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/baz.html-ac582de8.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-1417a332.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/ray.html-b3bc2326.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-27ab7387.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/disable.html-8ae7ae95.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/encrypt.html-e5535350.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/markdown.html-8102425b.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/page.html-014f0d8e.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-2a675fdc.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-205188aa.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/baz.html-08df27d4.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-602dd568.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/ray.html-813c0004.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index.html-a50be759.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/404.html-ae7c56df.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/auto-fe80bb03.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/index-2bf332f6.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/flowchart-c441f34d.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/mermaid.core-ab541ee6.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/highlight.esm-75b11b9d.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/markdown.esm-9d5bc2ce.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/math.esm-70a288c8.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/notes.esm-a106bb2c.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/reveal.esm-1a4c3ae7.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/search.esm-7e6792e2.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/zoom.esm-b83b91d0.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/VuePlayground-42588417.js" as="script"><link rel="prefetch" href="/AIGCDetect/assets/Valine.min-a8f57e06.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/AIGCDetect/"><img class="vp-nav-logo" src="/AIGCDetect/logo_MAS.png" alt><!----><!----></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Home" class="vp-link nav-link nav-link" href="/AIGCDetect/"><span class="font-icon icon iconfont icon-home" style=""></span>Home<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Evaluation" class="vp-link nav-link nav-link" href="/AIGCDetect/data/"><span class="font-icon icon iconfont icon-table" style=""></span>Evaluation<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Doc" class="vp-link nav-link active nav-link active" href="/AIGCDetect/Document.html"><span class="font-icon icon iconfont icon-page" style=""></span>Doc<!----></a></div><div class="nav-item hide-in-mobile"><a href="https://github.com/Ekko-zn/AIGCDetectBenchmark" rel="noopener noreferrer" target="_blank" aria-label="Code" class="nav-link"><span class="font-icon icon iconfont icon-github" style=""></span>Code<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Awesome" class="vp-link nav-link nav-link" href="/AIGCDetect/Awesome-AIGCDetection.html"><span class="font-icon icon iconfont icon-blog" style=""></span>Awesome<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Team" class="vp-link nav-link nav-link" href="/AIGCDetect/team/"><span class="font-icon icon iconfont icon-people" style=""></span>Team<!----></a></div></nav><!----><!----><!----><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Experiment Document</h1><!----><hr></div><!----><!----><div class="theme-hope-content"><div class="vp-tabs"><div class="vp-tabs-nav" role="tablist"><button type="button" class="vp-tab-nav active" role="tab" aria-controls="tab-1-0" aria-selected="true">Training Dataset</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="tab-1-1" aria-selected="false">Test Dataset</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="tab-1-2" aria-selected="false">Detection Method</button></div><!--[--><div class="vp-tab active" id="tab-1-0" role="tabpanel" aria-expanded="true"><p>In this benchmark, we adopt the training set in <a href="https://peterwang512.github.io/CNNDetection/">CNNSpot</a>, which contains <b>360K real images from LSUN and 360K fake images generated by ProGAN</b>. The whole dataset is divided into 20 different classes as shown bellow and every image is <b>256×256</b>. For a fair comparison of the generalization, all baselines (except for DIRE-D) are trained over this dataset. DIRE-D is a pre-trained detector trained over ADM dataset and its checkpoint is provided by their official codes.</p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/progan/00002.png" height="100" width="100"><figcaption>airplane</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00004.png" height="100px" width="100px"><figcaption>bird</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00011.png" height="100px" width="100px"><figcaption>bus</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00012.png" height="100px" width="100px"><figcaption>bicycle</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00032.png" height="100px" width="100px"><figcaption>bottle</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00015.png" height="100px" width="100px"><figcaption>boat</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00036.png" height="100px" width="100px"><figcaption>car</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00090.png" height="100px" width="100px"><figcaption>cat</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00105.png" height="100px" width="100px"><figcaption>chair</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00142.png" height="100px" width="100px"><figcaption>cow</figcaption></figure></div><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/progan/00136.png" height="100" width="100"><figcaption>diningtable</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00154.png" height="100px" width="100px"><figcaption>dog</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00159.png" height="100px" width="100px"><figcaption>horse</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00200.png" height="100px" width="100px"><figcaption>motorbike</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00416.png" height="100px" width="100px"><figcaption>person</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00018.png" height="100px" width="100px"><figcaption>pottedplant</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00074.png" height="100px" width="100px"><figcaption>sheep</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00027.png" height="100px" width="100px"><figcaption>sofa</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00084.png" height="100px" width="100px"><figcaption>train</figcaption></figure><figure><img src="/AIGCDetect/generated-images/progan/00071.png" height="100px" width="100px"><figcaption>tvmonitor</figcaption></figure></div><p></p></div><div class="vp-tab" id="tab-1-1" role="tabpanel" aria-expanded="false"><div class="vp-card-container"><a href="https://arxiv.org/pdf/1710.10196.pdf%C2%A0" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【ProGAN】 Progressive growing of gans for improved quality, stability, and variation</div><hr><div class="vp-card-desc"><p>Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.</p> <p><em>International Conference on Learning Representations,</em>  (<b>ICLR</b>). 2018.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/progan/00002.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/progan/00004.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00012.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00015.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00032.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00011.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00036.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00090.png" height="100px" width="100px"></figure></div></p><p>Unconditional GAN; image size：256×256; test number：8.0k;</p><p> classes：airplane, bird, bicycle, boat, bottle, bus, car, cat, cow, chair, diningtable, dog, person, pottedplant, motorbike, tvmonitor, train, sheep, sofa, horse</p></div></div></a><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【StyleGAN】A style-based generator architecture for generative adversarial networks</div><hr><div class="vp-card-desc"><p>Tero Karras, Samuli Laine, and Timo Aila.</p> <p><em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,</em>  (<b>CVPR</b>). 2019.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/stylegan/001083.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/stylegan/005960.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/071371.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/071761.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/003526.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/003842.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/007620.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/072893.png" height="100px" width="100px"></figure></div></p><p>Unconditional GAN; image size：256×256; test number：12.0k; classes：cat, car, bedroom</p></div></div></a><a href="https://arxiv.org/pdf/1809.11096.pdf%20http://arxiv.org/abs/1809.11096.pdf" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【BigGAN】Large scale gan training for high fidelity natural image synthesis</div><hr><div class="vp-card-desc"><p>Andrew Brock，Jeff Donahue, and Karen Simonyan.</p> <p><em>International Conference on Learning Representations,</em>  (<b>ICLR</b>). 2018.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/biggan/00003716.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/biggan/00027048.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00036692.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00118802.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00168103.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00172170.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00199642.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00145596.png" height="100px" width="100px"></figure></div></p><p>Unconditional GAN; image size：256×256; test number：4.0k; classes：the same as ImageNet</p></div></div></a><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【CycleGAN】Unpaired image-to-image translation using cycleconsistent adversarial networks</div><hr><div class="vp-card-desc"><p>Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros.</p> <p><em>Proceedings of the IEEE international conference on computer vision,</em>  (<b>ICCV</b>). 2017.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/cyclegan/n07749192_401_fake.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/cyclegan/n02391049_400_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/n07740461_20_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/2008-07-09 20_32_51_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/2011-05-28 15_13_21_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/n02381460_140_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/n02381460_440_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/n07740461_451_fake.png" height="100px" width="100px"></figure></div></p><p>Conditional GAN; image size：256×256; test number：2.6k; classes：apple, horse, orange, summer, winter, zebra</p></div></div></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【StarGAN】Stargan： Unified generative adversarial networks for multi-domain image-to-image translation</div><hr><div class="vp-card-desc"><p>Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo.</p> <p><em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2018.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/stargan/0000.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/stargan/0001.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0002.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0003.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0004.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0005.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0006.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0007.png" height="100px" width="100px"></figure></div></p><p>Conditional GAN; image size：256×256; test number：4.0k; classes：person</p></div></div></a><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【GauGAN】Semantic image synthesis with spatially-adaptive normalization</div><hr><div class="vp-card-desc"><p>Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,</em>  (<b>CVPR</b>). 2019.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/gaugan/000000000872.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/gaugan/000000000139.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000285.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000632.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000724.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000776.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000785.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000802.png" height="100px" width="100px"></figure></div></p><p>Conditional GAN; image size：256×256; test number：10.0k; classes：multi</p></div></div></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.pdf" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【StyleGAN2】Analyzing and improving the image quality of stylegan</div><hr><div class="vp-card-desc"><p>Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,</em>  (<b>CVPR</b>). 2020.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/stylegan2/000000.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/stylegan2/000001.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000002.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000003.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000004.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000005.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000006.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000007.png" height="100px" width="100px"></figure></div></p><p>Unconditional GAN; image size：256×256; test number：15.9k; classes：car, cat, church, horse</p></div></div></a><a href="https://www.whichfaceisreal.com/" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">whichfaceisreal.com</div><hr><div class="vp-card-desc"><p> Jevin West and Carl Bergstrom.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000933.jpeg" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-16_235902.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000020.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000907.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000914.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000920.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000927.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000901.jpeg" height="100px" width="100px"></figure></div></p><p>Conditional GAN; image size：1024×1024; test number：2.0k; classes：person</p></div></div></a><a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【ADM】Diffusion Models Beat GANs on Image Synthesis</div><hr><div class="vp-card-desc"><p>Prafulla Dhariwal and Alex Nichol.</p> <p><em>Advances in neural information processing systems,</em>  (<b>neurips</b>). 2021.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/ADM/1_adm_34.PNG" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/ADM/2_adm_85.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/24_adm_7.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/136_adm_7.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/151_adm_85.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/331_adm_85.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/348_adm_174.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/425_adm_91.PNG" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：256×256; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p></div></div></a><a href="https://arxiv.org/abs/2112.10741" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【Glide】GLIDE： Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</div><hr><div class="vp-card-desc"><p> JAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.</p> <p><em>Advances in neural information processing systems,</em>  (<b>neurips</b>). 2021.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_00_001_glide_00039.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_00_015_glide_00039.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_01_152_glide_00074.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_01_168_glide_00039.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_02_248_glide_00039.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_02_281_glide_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_03_351_glide_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_05_509_glide_00035.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：256×256; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p></div></div></a><a href="https://www.midjourney.com/home/" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">Midjourney</div><hr><div class="vp-card-desc"><p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/Midjourney/1_midjourney_169.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/Midjourney/370_midjourney_88.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/19_midjourney_197.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/163_midjourney_100.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/406_midjourney_100.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/281_midjourney_100.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/340_midjourney_198.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/407_midjourney_34.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：1024×1024; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p></div></div></a><a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【Stable Diffusion v1.4】High-Resolution Image Synthesis with Latent Diffusion Models</div><hr><div class="vp-card-desc"><p> JRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2022.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/SDv14/001_sdv4_00127.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/SDv14/010_sdv4_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/036_sdv4_00074.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/133_sdv4_00143.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/153_sdv4_00127.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/296_sdv4_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/468_sdv4_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/624_sdv4_00035.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：512×512; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p></div></div></a><a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【Stable Diffusion v1.5】High-Resolution Image Synthesis with Latent Diffusion Models</div><hr><div class="vp-card-desc"><p> JRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2022.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/SDv15/001_sdv5_00020.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/SDv15/008_sdv5_00009.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/162_sdv5_00020.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/283_sdv5_00135.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/340_sdv5_00195.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/407_sdv5_00135.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/424_sdv5_00100.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/436_sdv5_00020.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：512×512; test number：16.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p></div></div></a><a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Vector_Quantized_Diffusion_Model_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【VQDM】Vector Quantized Diffusion Model for Text-to-Image Synthesis</div><hr><div class="vp-card-desc"><p> Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2022.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_00_001_vqdm_00035.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_00_008_vqdm_00143.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_01_153_vqdm_00143.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_02_284_vqdm_00143.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_04_454_vqdm_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_03_339_vqdm_00020.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_04_407_vqdm_00020.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_00_099_vqdm_00074.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：256×256; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p></div></div></a><a href="https://xihe.mindspore.cn/modelzoo/wukong. 2022" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">Wukong</div><hr><div class="vp-card-desc"><p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/wukong/1_wukong_image88.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/wukong/17_wukong_image128.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/165_wukong_image88.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/281_wukong_image128.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/325_wukong_image61.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/444_wukong_image88.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/661_wukong_image128.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/766_wukong_image188.png" height="100px" width="100px"></figure></div></p><p>StyleGAN; image size：512×512; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p></div></div></a><a href="https://3dvar.com/Ramesh2022Hierarchical.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【DALL·E 2】Hierarchical Text-Conditional Image Generation with CLIP Latents</div><hr><div class="vp-card-desc"><p> Aditya Ramesh， Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.<p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/DALLE2/0098.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/DALLE2/0210.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0381.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0559.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0591.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0764.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0810.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0915.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：256×256; test number：2.0k; input sentences follow the captions of <a href="https://cocodataset.org/#home">COCO dataset</a></div></div></a><a href="https://3dvar.com/Ramesh2022Hierarchical.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/None"><div class="vp-card-content"><div class="vp-card-title">【SDXL】 Sdxl：Improving latent diffusion models for high-resolution image synthesis</div><hr><div class="vp-card-desc"><p> Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach.<p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_28506.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/sdxl/sdxl_28758.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_32054.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_32203.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_34904.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_52147.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_66827.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_90753.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：1024×1024; test number：4.0k; input sentences follow the captions of <a href="https://cocodataset.org/#home">COCO dataset</a></div></div></a><!-- <table id="dataset" class="mytable2">
    <tr>
        <th>Generative Model Type</th>
        <th>Generative Method</th>
        <th>size</th>
        <th>images</th>
        <th>source</th>
        <th>classes</th>
        <th >about</th>
    </tr>
    <tr>
        <td >Unconditional GAN</td>
        <td>ProGAN</td>
        <td >256×256</td>
        <td>8.0k</td>
        <td>LSUN</td>
        <td>20</td>
        <td></td>
    </tr>
    <tr>
        <td >Unconditional GAN</td>
        <td>StyleGAN</td>
        <td >256×256</td>
        <td>12.0k</td>
        <td>LSUN</td>
        <td>3</td>
        <td>injects large, per-pixel noise into the model to introduce high frequency detail</td>
    </tr>
    <tr>
        <td >Unconditional GAN</td>
        <td>BigGAN</td>
        <td >256×256</td>
        <td>4.0k</td>
        <td>ImageNet</td>
        <td>multi</td>
        <td>has a monolithic, classconditional structure, is trained on very large batch sizes, and uses self-attention layers</td>
    </tr>
    <tr>
        <td >Unconditional GAN</td>
        <td>StyleGAN2</td>
        <td >256×256</td>
        <td>15.9k</td>
        <td>LSUN</td>
        <td>4</td>
        <td></td>
    </tr>
    <tr>
        <td >Conditional GAN</td>
        <td>CycleGAN</td>
        <td >256×256</td>
        <td>2.6k</td>
        <td>Style/object transfer</td>
        <td>6</td>
        <td></td>
    </tr>
    <tr>
        <td >Conditional GAN</td>
        <td>StarGAN</td>
        <td >256×256</td>
        <td>4.0k</td>
        <td>CelebA</td>
        <td>multi</td>
        <td></td>
    </tr>
    <tr>
        <td >Conditional GAN</td>
        <td>GauGAN</td>
        <td >256×256</td>
        <td>10.0k</td>
        <td>COCO</td>
        <td>multi</td>
        <td></td>
    </tr>
    <tr>
        <td>Deepfake</td>
        <td>Deepfake (FaceForensics++)</td>
        <td >256×256</td>
        <td>5.4k</td>
        <td>Videos of faces</td>
        <td>multi</td>
        <td>uses an autoencoder to generate faces, and images undergo extensive post-processing steps, including Poisson image blending [30] with real content</td>
    </tr>
    <tr>
        <td>Unknown</td>
        <td>whichfaceisreal</td>
        <td>1024×1024</td>
        <td>2.0k</td>
        <td>/</td>
        <td>multi</td>
        <td>whichfaceisreal.com</td>
    </tr>
    <tr>
        <td>Diffusion</td>
        <td>ADM</td>
        <td>256×256</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>with classifier guidance, which is pretrained on ImageNet</td>
    </tr>
    <tr>
        <td>Diffusion U-net</td>
        <td>Glide</td>
        <td>256×256</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>with classifier-free guidance</td>
    </tr>
    <tr>
        <td>Unknown</td>
        <td>Midjourney v5</td>
        <td>1024×1024</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>one of the most renowned commercial software programs, known for its exceptional image generation performance.</td>
    </tr>
    <tr>
        <td >Diffusion+Decoder</td>
        <td>Stable Diffusion v1.4</td>
        <td>512×512</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>pretrained from the Stable Diffusion V1.2 checkpoint and fine-tuned on 225k steps at resolution 512x512 on Laion-Aesthetics V2 5+ dataset and drops 10% of the text-conditioning</td>
    </tr>
    <tr>
        <td >Diffusion+Decoder</td>
        <td>Stable Diffusion v1.5</td>
        <td>512×512</td>
        <td>16.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>training settings is the same as V1.4; is fine-tuned on 595,000 steps</td>
    </tr>
    <tr>
        <td>Diffusion</td>
        <td>VQDM</td>
        <td>256×256</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td></td>
    </tr>
    <tr>
        <td>Unknown</td>
        <td>wukong</td>
        <td>512×512</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>trained on the largest Chinese open-source multimodal dataset</td>
    </tr>
    <tr>
        <td>Diffusion U-net</td>
        <td>DALL·E2</td>
        <td>256×256</td>
        <td>2k</td>
        <td></td>
        <td>multi</td>
        <td>generating images with COCO text</td>
    </tr>
    <tr>
        <td>Diffusion U-net</td>
        <td>DDPM</td>
        <td>256×256</td>
        <td>6k</td>
        <td>LSUN/celebahq</td>
        <td>3</td>
        <td>null</td>
    </tr>
</table> --></div></div><div class="vp-tab" id="tab-1-2" role="tabpanel" aria-expanded="false"><div class="vp-card-container"><a href="https://peterwang512.github.io/CNNDetection/" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/CNNSpot.png"><div class="vp-card-content"><div class="vp-card-title">CNN-generated images are surprisingly easy to spot... for now <b>[CNNSpot]</b></div><hr><div class="vp-card-desc"><p>Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A. Efros.</p> <p><em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,</em>  (<b>CVPR</b>). 2020.</p> <hr /><p>CNNSpot proposes a simple yet effective fake image detector. They adopt ResNet-50 as a classifier and observe that data augmentation, including JPEG compression and Gaussian blur, can boost the generalization of the detector, which means the detector can generalize well to unseen architectures, datasets, and training methods.</p></div></div></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Global_Texture_Enhancement_for_Fake_Face_Detection_in_the_Wild_CVPR_2020_paper.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/Gram.png"><div class="vp-card-content"><div class="vp-card-title">Global Texture Enhancement for Fake Face Detection In the Wild <b>[GramNet]</b></div><hr><div class="vp-card-desc"><p>Zhengzhe Liu, Xiaojuan Qi, and Philip H. S. Torr.</p> <p><em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,</em>  (<b>CVPR</b>). 2020.</p> <hr /><p>GramNet abserves the difference between the texture of fake faces and real ones. Motivated by the observation, they aim to improve the generalization and robustness of the detector by incorporating a global texture extraction into the common ResNet structure.</p></div></div></a><a href="https://proceedings.mlr.press/v119/frank20a/frank20a.pdf" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/FreDect.png"><div class="vp-card-content"><div class="vp-card-title">Leveraging Frequency Analysis for Deep Fake Image Recognition <b>[FreDect]</b></div><hr><div class="vp-card-desc"><p>Joel Frank, Thorsten Eisenhofer, Lea Sch ̈onherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz.</p> <p><em>International conference on machine learning,</em> (<b>PMLR</b>). 2020.</p> <hr /><p>FreDect reveals that in frequency space, GAN-generated images exhibit severe artifacts that can be easily identified. Based on this analysis, they propose the frequency abnormality of fake images and conducts fake image detection from the frequency domain.</p></div></div></a><a href="https://arxiv.org/pdf/2203.13964.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/Fusing.png"><div class="vp-card-content"><div class="vp-card-title">Fusing global and local features for generalized ai-synthesized image detection <b>[Fusing]</b></div><hr><div class="vp-card-desc"><p>Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki Nagano‡, and Siwei Lyu.</p> <p><em>2022 IEEE International Conference on Image Processing, </em> (<b>ICIP</b>). 2022.</p> <hr /><p>Fusing uses a two-branch model to extract global spatial information from the whole image and local informative features from multiple patches selected by a novel patch selection module. Global and local features are fused by the Multi-head attention mechanism. Then, a classifier is trained to detect fake image based on the fused feature.</p></div></div></a><a href="https://dl.acm.org/doi/abs/10.1007/978-3-031-19781-9_6" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/LNP.png"><div class="vp-card-content"><div class="vp-card-title">Detecting Generated Images by Real Images <b>[LNP]</b></div><hr><div class="vp-card-desc"><p>Bo Liu, Fan Yang, Xiuli Bi, Bin Xiao, Weisheng Li, and Xinbo Gao.</p> <p><em>European Conference on Computer Vision,</em> (<b>ECCV</b>). 2022.</p> <hr /><p>LNP observed that the noise pattern of real images exhibits similar characteristics in the frequency domain, while the generated images are far different. Therefore, it extracts the noise pattern of spatial images based on a well-trained denoising model. Then, it identifies fake images from the frequency domain of the noise pattern.</p></div></div></a><a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Learning_on_Gradients_Generalized_Artifacts_Representation_for_GAN-Generated_Images_Detection_CVPR_2023_paper.pdf" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/Grad.png"><div class="vp-card-content"><div class="vp-card-title">Learning on Gradients： Generalized Artifacts Representation for GAN-Generated Images Detection <b>[LGrad]</b></div><hr><div class="vp-card-desc"><p>Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, and Yunchao Wei.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2023.</p> <hr /><p>LGrad extracts gradient map, which is obtained by a well-trained image classifier, as the fingerprint of an GAN-generated image. This approach turns the data-dependent problem into a transformationmodel-dependent problem. Then, it conducts a binary classification task based on gradient maps.</p></div></div></a><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/UnivFD.png"><div class="vp-card-content"><div class="vp-card-title">Towards Universal Fake Image Detectors that Generalize Across Generative Models <b>[UnivFD]</b></div><hr><div class="vp-card-desc"><p>Utkarsh Ojha, Yuheng Li, and Yong Jae Lee.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em> (<b>CVPR</b>).2023.</p> <hr /><p>UnivFD uses a feature space extracted by a large pre-trained vision-language model (CLIP:ViT-L/14) to train the detector. The large pre-trained model leads to a smooth decision boundary, which improves the generalization of the detector</p></div></div></a><a href="https://arxiv.org/pdf/2303.09295" target="_blank" class="vp-card" style="background:rgba(253, 230, 138, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/DIRE.png"><div class="vp-card-content"><div class="vp-card-title">DIRE for Diffusion-Generated Image Detection <b>[DIRE]</b></div><hr><div class="vp-card-desc"><p>Zhendong Wang, Jianmin Bao, Wengang Zhou,Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li.</p> <p><em>International Conference on Computer Vision,</em> (<b>ICCV</b>).  2023.</p> <hr /><p>DIRE is dedicated to identifying fake images generated by diffusion-based images. They observe that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. Based on this observation, they leverage the error between an input image and its reconstruction counterpart by a pre-trained diffusion model as a fingerprint.</p></div></div></a><a href="https://arxiv.org/abs/2311.12397" target="_blank" class="vp-card" style="background:rgba(172, 216, 255, 0.15);"><img class="vp-card-logo" src="/AIGCDetect/RPTC.png"><div class="vp-card-content"><div class="vp-card-title"><font color="red">PatchCraft：Exploring Texture Patch for Efficient AI-generated Image Detection</font> <b>[PatchCraft (RPTC)]</b></div><hr><div class="vp-card-desc"><p>Nan Zhong, Yiran Xu, Zhenxing Qian, and Xinpeng Zhang.</p><hr /><p>PatchCraft leverages the inter-pixel correlation contrast between rich and poor texture regions within an image. Pixels in rich texture regions exhibit more significant fluctuations than those in poor texture regions. Based on this principle, we divide an image into multiple patches and reconstruct them into two images, comprising rich-texture and poor-texture patches respectively. Subsequently, we extract the inter-pixel correlation discrepancy feature between rich and poor texture regions. This feature serves as a universal fingerprint used for AI-generated image forensics across different generative models. </p></div></div></a></div></div><!--]--></div><!-- <table id="detection" class="mytable2">
        <tr>
            <th>Detection Method</th>
            <th>target model</th>
            <th>training set</th>
            <th>brief</th>
        </tr>
        <tr>
            <td>CNNSpot</td>
            <td>GAN</td>
            <td>progan</td>
            <td>train a ResNet50 classifier</td>
        </tr>
        <tr>
            <td>DCTAnalysis</td>
            <td>GAN</td>
            <td>progan</td>
            <td>perform a ridge regression on real and generated images, after applying a DCT</td>
        </tr>
        <tr>
            <td>PSM</td>
            <td>GAN</td>
            <td>progan</td>
            <td>combine global features from the whole image with the local features from informative patches</td>
        </tr>
        <tr>
            <td>Gram-Net</td>
            <td>GAN</td>
            <td>progan</td>
            <td>train a Gram-Net classifier</td>
        </tr>
        <tr>
            <td>LGrad</td>
            <td>GAN</td>
            <td>progan</td>
            <td>transform the images to the gradients by a pretrained CNN model, then train a ResNet50 classifier</td>
        </tr>
        <tr>
            <td>LNP-based</td>
            <td>GAN/Glow</td>
            <td>progan</td>
            <td>use the LNP(Learned Noise Patterns) to detect fake images with a ResNet50 classifier</td>
        </tr>
        <tr>
            <td>DIRE(lsun_iddpm)</td>
            <td>Diffusion</td>
            <td>ADM</td>
            <td>calculate the differences between origin image and its recovered version by a pretrained diffusion model</td>
        </tr>
        <tr>
            <td>DIRE(progan)</td>
            <td>Diffusion</td>
            <td>progan</td>
            <td></td>
        </tr>
        <tr>
            <td>RPTC</td>
            <td>All</td>
            <td>progan</td>
            <td></td>
        </tr>
</table> --></div><!----><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><!----><!----><!----><!--]--></main><!--]--><!----></div><!--]--><!----><!--]--></div>
    <script type="module" src="/AIGCDetect/assets/app-d6f521c8.js" defer></script>
  </body>
</html>
