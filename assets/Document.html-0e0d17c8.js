import{_ as h,r as o,o as l,c as m,d as t,w as r,e as p,b as a,a as e,n,g}from"./app-d6f521c8.js";const u="/AIGCDetect/generated-images/progan/00002.png",f="/AIGCDetect/generated-images/progan/00004.png",_="/AIGCDetect/generated-images/progan/00011.png",x="/AIGCDetect/generated-images/progan/00012.png",D="/AIGCDetect/generated-images/progan/00032.png",C="/AIGCDetect/generated-images/progan/00015.png",w="/AIGCDetect/generated-images/progan/00036.png",b="/AIGCDetect/generated-images/progan/00090.png",G="/AIGCDetect/generated-images/progan/00105.png",A="/AIGCDetect/generated-images/progan/00142.png",I="/AIGCDetect/generated-images/progan/00136.png",v="/AIGCDetect/generated-images/progan/00154.png",y="/AIGCDetect/generated-images/progan/00159.png",k="/AIGCDetect/generated-images/progan/00200.png",N="/AIGCDetect/generated-images/progan/00416.png",P="/AIGCDetect/generated-images/progan/00018.png",V="/AIGCDetect/generated-images/progan/00074.png",L="/AIGCDetect/generated-images/progan/00027.png",E="/AIGCDetect/generated-images/progan/00084.png",R="/AIGCDetect/generated-images/progan/00071.png";const S={},M=e("p",null,[a("In this benchmark, we adopt the training set in "),e("a",{href:"https://peterwang512.github.io/CNNDetection/"},"CNNSpot"),a(", which contains "),e("b",null,"360K real images from LSUN and 360K fake images generated by ProGAN"),a(". The whole dataset is divided into 20 different classes as shown bellow and every image is "),e("b",null,"256×256"),a(". For a fair comparison of the generalization, all baselines (except for DIRE-D) are trained over this dataset. DIRE-D is a pre-trained detector trained over ADM dataset and its checkpoint is provided by their official codes.")],-1),T=e("div",{class:"image-container2"},[e("figure",null,[e("img",{src:u,height:"100",width:"100"}),e("figcaption",null,"airplane")]),e("figure",null,[e("img",{src:f,height:"100px",width:"100px"}),e("figcaption",null,"bird")]),e("figure",null,[e("img",{src:_,height:"100px",width:"100px"}),e("figcaption",null,"bus")]),e("figure",null,[e("img",{src:x,height:"100px",width:"100px"}),e("figcaption",null,"bicycle")]),e("figure",null,[e("img",{src:D,height:"100px",width:"100px"}),e("figcaption",null,"bottle")]),e("figure",null,[e("img",{src:C,height:"100px",width:"100px"}),e("figcaption",null,"boat")]),e("figure",null,[e("img",{src:w,height:"100px",width:"100px"}),e("figcaption",null,"car")]),e("figure",null,[e("img",{src:b,height:"100px",width:"100px"}),e("figcaption",null,"cat")]),e("figure",null,[e("img",{src:G,height:"100px",width:"100px"}),e("figcaption",null,"chair")]),e("figure",null,[e("img",{src:A,height:"100px",width:"100px"}),e("figcaption",null,"cow")])],-1),z=e("div",{class:"image-container2"},[e("figure",null,[e("img",{src:I,height:"100",width:"100"}),e("figcaption",null,"diningtable")]),e("figure",null,[e("img",{src:v,height:"100px",width:"100px"}),e("figcaption",null,"dog")]),e("figure",null,[e("img",{src:y,height:"100px",width:"100px"}),e("figcaption",null,"horse")]),e("figure",null,[e("img",{src:k,height:"100px",width:"100px"}),e("figcaption",null,"motorbike")]),e("figure",null,[e("img",{src:N,height:"100px",width:"100px"}),e("figcaption",null,"person")]),e("figure",null,[e("img",{src:P,height:"100px",width:"100px"}),e("figcaption",null,"pottedplant")]),e("figure",null,[e("img",{src:V,height:"100px",width:"100px"}),e("figcaption",null,"sheep")]),e("figure",null,[e("img",{src:L,height:"100px",width:"100px"}),e("figcaption",null,"sofa")]),e("figure",null,[e("img",{src:E,height:"100px",width:"100px"}),e("figcaption",null,"train")]),e("figure",null,[e("img",{src:R,height:"100px",width:"100px"}),e("figcaption",null,"tvmonitor")])],-1),j=e("p",null,null,-1),F={class:"vp-card-container"},U={class:"vp-card-container"};function B(Q,J){const i=o("VPCard"),c=o("Tabs");return l(),m("div",null,[t(c,{id:"1",data:[{id:"Training Dataset"},{id:"Test Dataset"},{id:"Detection Method"}]},{title0:r(({value:s,isActive:d})=>[a("Training Dataset")]),title1:r(({value:s,isActive:d})=>[a("Test Dataset")]),title2:r(({value:s,isActive:d})=>[a("Detection Method")]),tab0:r(({value:s,isActive:d})=>[M,T,z,j]),tab1:r(({value:s,isActive:d})=>[e("div",F,[t(i,n(g({title:"【ProGAN】 Progressive growing of gans for improved quality, stability, and variation",desc:'<p>Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.</p> <p><em>International Conference on Learning Representations,</em>  (<b>ICLR</b>). 2018.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/progan/00002.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/progan/00004.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00012.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00015.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00032.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00011.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00036.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/progan/00090.png" height="100px" width="100px"></figure></div></p><p>Unconditional GAN; image size：256×256; test number：8.0k;</p><p> classes：airplane, bird, bicycle, boat, bottle, bus, car, cat, cow, chair, diningtable, dog, person, pottedplant, motorbike, tvmonitor, train, sheep, sofa, horse</p>',link:"https://arxiv.org/pdf/1710.10196.pdf%C2%A0",color:"rgba(172, 216, 255, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【StyleGAN】A style-based generator architecture for generative adversarial networks",desc:'<p>Tero Karras, Samuli Laine, and Timo Aila.</p> <p><em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,</em>  (<b>CVPR</b>). 2019.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/stylegan/001083.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/stylegan/005960.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/071371.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/071761.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/003526.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/003842.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/007620.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan/072893.png" height="100px" width="100px"></figure></div></p><p>Unconditional GAN; image size：256×256; test number：12.0k; classes：cat, car, bedroom</p>',link:"https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf",color:"rgba(253, 230, 138, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【BigGAN】Large scale gan training for high fidelity natural image synthesis",desc:'<p>Andrew Brock，Jeff Donahue, and Karen Simonyan.</p> <p><em>International Conference on Learning Representations,</em>  (<b>ICLR</b>). 2018.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/biggan/00003716.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/biggan/00027048.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00036692.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00118802.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00168103.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00172170.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00199642.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/biggan/00145596.png" height="100px" width="100px"></figure></div></p><p>Unconditional GAN; image size：256×256; test number：4.0k; classes：the same as ImageNet</p>',link:"https://arxiv.org/pdf/1809.11096.pdf%20http://arxiv.org/abs/1809.11096.pdf",color:"rgba(172, 216, 255, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【CycleGAN】Unpaired image-to-image translation using cycleconsistent adversarial networks",desc:'<p>Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros.</p> <p><em>Proceedings of the IEEE international conference on computer vision,</em>  (<b>ICCV</b>). 2017.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/cyclegan/n07749192_401_fake.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/cyclegan/n02391049_400_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/n07740461_20_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/2008-07-09 20_32_51_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/2011-05-28 15_13_21_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/n02381460_140_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/n02381460_440_fake.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/cyclegan/n07740461_451_fake.png" height="100px" width="100px"></figure></div></p><p>Conditional GAN; image size：256×256; test number：2.6k; classes：apple, horse, orange, summer, winter, zebra</p>',link:"http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf",color:"rgba(253, 230, 138, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【StarGAN】Stargan： Unified generative adversarial networks for multi-domain image-to-image translation",desc:'<p>Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo.</p> <p><em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2018.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/stargan/0000.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/stargan/0001.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0002.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0003.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0004.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0005.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0006.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stargan/0007.png" height="100px" width="100px"></figure></div></p><p>Conditional GAN; image size：256×256; test number：4.0k; classes：person</p>',link:"http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf",color:"rgba(172, 216, 255, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【GauGAN】Semantic image synthesis with spatially-adaptive normalization",desc:'<p>Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,</em>  (<b>CVPR</b>). 2019.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/gaugan/000000000872.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/gaugan/000000000139.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000285.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000632.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000724.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000776.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000785.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/gaugan/000000000802.png" height="100px" width="100px"></figure></div></p><p>Conditional GAN; image size：256×256; test number：10.0k; classes：multi</p>',link:"http://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.pdf",color:"rgba(253, 230, 138, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【StyleGAN2】Analyzing and improving the image quality of stylegan",desc:'<p>Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,</em>  (<b>CVPR</b>). 2020.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/stylegan2/000000.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/stylegan2/000001.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000002.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000003.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000004.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000005.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000006.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/stylegan2/000007.png" height="100px" width="100px"></figure></div></p><p>Unconditional GAN; image size：256×256; test number：15.9k; classes：car, cat, church, horse</p>',link:"https://openaccess.thecvf.com/content_CVPR_2020/papers/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.pdf",color:"rgba(172, 216, 255, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"whichfaceisreal.com",desc:'<p> Jevin West and Carl Bergstrom.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000933.jpeg" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-16_235902.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000020.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000907.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000914.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000920.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000927.jpeg" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/whichfaceisreal/image-2019-02-17_000901.jpeg" height="100px" width="100px"></figure></div></p><p>Conditional GAN; image size：1024×1024; test number：2.0k; classes：person</p>',link:"https://www.whichfaceisreal.com/",color:"rgba(253, 230, 138, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【ADM】Diffusion Models Beat GANs on Image Synthesis",desc:'<p>Prafulla Dhariwal and Alex Nichol.</p> <p><em>Advances in neural information processing systems,</em>  (<b>neurips</b>). 2021.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/ADM/1_adm_34.PNG" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/ADM/2_adm_85.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/24_adm_7.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/136_adm_7.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/151_adm_85.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/331_adm_85.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/348_adm_174.PNG" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/ADM/425_adm_91.PNG" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：256×256; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p>',link:"https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf",color:"rgba(172, 216, 255, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【Glide】GLIDE： Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",desc:'<p> JAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.</p> <p><em>Advances in neural information processing systems,</em>  (<b>neurips</b>). 2021.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_00_001_glide_00039.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_00_015_glide_00039.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_01_152_glide_00074.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_01_168_glide_00039.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_02_248_glide_00039.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_02_281_glide_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_03_351_glide_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Glide/GLIDE_1000_200_05_509_glide_00035.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：256×256; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p>',link:"https://arxiv.org/abs/2112.10741",color:"rgba(253, 230, 138, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"Midjourney",desc:'<p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/Midjourney/1_midjourney_169.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/Midjourney/370_midjourney_88.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/19_midjourney_197.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/163_midjourney_100.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/406_midjourney_100.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/281_midjourney_100.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/340_midjourney_198.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/Midjourney/407_midjourney_34.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：1024×1024; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p>',link:"https://www.midjourney.com/home/",color:"rgba(172, 216, 255, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【Stable Diffusion v1.4】High-Resolution Image Synthesis with Latent Diffusion Models",desc:'<p> JRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2022.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/SDv14/001_sdv4_00127.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/SDv14/010_sdv4_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/036_sdv4_00074.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/133_sdv4_00143.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/153_sdv4_00127.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/296_sdv4_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/468_sdv4_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv14/624_sdv4_00035.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：512×512; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p>',link:"http://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf",color:"rgba(253, 230, 138, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【Stable Diffusion v1.5】High-Resolution Image Synthesis with Latent Diffusion Models",desc:'<p> JRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2022.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/SDv15/001_sdv5_00020.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/SDv15/008_sdv5_00009.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/162_sdv5_00020.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/283_sdv5_00135.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/340_sdv5_00195.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/407_sdv5_00135.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/424_sdv5_00100.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/SDv15/436_sdv5_00020.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：512×512; test number：16.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p>',link:"http://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf",color:"rgba(172, 216, 255, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【VQDM】Vector Quantized Diffusion Model for Text-to-Image Synthesis",desc:'<p> Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2022.</p> <p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_00_001_vqdm_00035.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_00_008_vqdm_00143.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_01_153_vqdm_00143.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_02_284_vqdm_00143.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_04_454_vqdm_00035.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_03_339_vqdm_00020.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_04_407_vqdm_00020.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/VQDM/VQDM_1000_200_00_099_vqdm_00074.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：256×256; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p>',link:"http://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Vector_Quantized_Diffusion_Model_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf",color:"rgba(253, 230, 138, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"Wukong",desc:'<p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/wukong/1_wukong_image88.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/wukong/17_wukong_image128.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/165_wukong_image88.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/281_wukong_image128.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/325_wukong_image61.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/444_wukong_image88.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/661_wukong_image128.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/wukong/766_wukong_image188.png" height="100px" width="100px"></figure></div></p><p>StyleGAN; image size：512×512; test number：12.0k; input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels</p>',link:"https://xihe.mindspore.cn/modelzoo/wukong. 2022",color:"rgba(172, 216, 255, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【DALL·E 2】Hierarchical Text-Conditional Image Generation with CLIP Latents",desc:'<p> Aditya Ramesh， Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.<p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/DALLE2/0098.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/DALLE2/0210.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0381.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0559.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0591.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0764.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0810.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/DALLE2/0915.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：256×256; test number：2.0k; input sentences follow the captions of <a href="https://cocodataset.org/#home">COCO dataset</a>',link:"https://3dvar.com/Ramesh2022Hierarchical.pdf",color:"rgba(253, 230, 138, 0.15)",logo:"None"})),null,16),t(i,n(g({title:"【SDXL】 Sdxl：Improving latent diffusion models for high-resolution image synthesis",desc:'<p> Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach.<p><div class="image-container2"><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_28506.png" height="100" width="100"></figure><figure> <img src="/AIGCDetect/generated-images/sdxl/sdxl_28758.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_32054.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_32203.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_34904.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_52147.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_66827.png" height="100px" width="100px"></figure><figure><img src="/AIGCDetect/generated-images/sdxl/sdxl_90753.png" height="100px" width="100px"></figure></div></p><p>Diffusion; image size：1024×1024; test number：4.0k; input sentences follow the captions of <a href="https://cocodataset.org/#home">COCO dataset</a>',link:"https://3dvar.com/Ramesh2022Hierarchical.pdf",color:"rgba(253, 230, 138, 0.15)",logo:"None"})),null,16),p(` <table id="dataset" class="mytable2">
    <tr>
        <th>Generative Model Type</th>
        <th>Generative Method</th>
        <th>size</th>
        <th>images</th>
        <th>source</th>
        <th>classes</th>
        <th >about</th>
    </tr>
    <tr>
        <td >Unconditional GAN</td>
        <td>ProGAN</td>
        <td >256×256</td>
        <td>8.0k</td>
        <td>LSUN</td>
        <td>20</td>
        <td></td>
    </tr>
    <tr>
        <td >Unconditional GAN</td>
        <td>StyleGAN</td>
        <td >256×256</td>
        <td>12.0k</td>
        <td>LSUN</td>
        <td>3</td>
        <td>injects large, per-pixel noise into the model to introduce high frequency detail</td>
    </tr>
    <tr>
        <td >Unconditional GAN</td>
        <td>BigGAN</td>
        <td >256×256</td>
        <td>4.0k</td>
        <td>ImageNet</td>
        <td>multi</td>
        <td>has a monolithic, classconditional structure, is trained on very large batch sizes, and uses self-attention layers</td>
    </tr>
    <tr>
        <td >Unconditional GAN</td>
        <td>StyleGAN2</td>
        <td >256×256</td>
        <td>15.9k</td>
        <td>LSUN</td>
        <td>4</td>
        <td></td>
    </tr>
    <tr>
        <td >Conditional GAN</td>
        <td>CycleGAN</td>
        <td >256×256</td>
        <td>2.6k</td>
        <td>Style/object transfer</td>
        <td>6</td>
        <td></td>
    </tr>
    <tr>
        <td >Conditional GAN</td>
        <td>StarGAN</td>
        <td >256×256</td>
        <td>4.0k</td>
        <td>CelebA</td>
        <td>multi</td>
        <td></td>
    </tr>
    <tr>
        <td >Conditional GAN</td>
        <td>GauGAN</td>
        <td >256×256</td>
        <td>10.0k</td>
        <td>COCO</td>
        <td>multi</td>
        <td></td>
    </tr>
    <tr>
        <td>Deepfake</td>
        <td>Deepfake (FaceForensics++)</td>
        <td >256×256</td>
        <td>5.4k</td>
        <td>Videos of faces</td>
        <td>multi</td>
        <td>uses an autoencoder to generate faces, and images undergo extensive post-processing steps, including Poisson image blending [30] with real content</td>
    </tr>
    <tr>
        <td>Unknown</td>
        <td>whichfaceisreal</td>
        <td>1024×1024</td>
        <td>2.0k</td>
        <td>/</td>
        <td>multi</td>
        <td>whichfaceisreal.com</td>
    </tr>
    <tr>
        <td>Diffusion</td>
        <td>ADM</td>
        <td>256×256</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>with classifier guidance, which is pretrained on ImageNet</td>
    </tr>
    <tr>
        <td>Diffusion U-net</td>
        <td>Glide</td>
        <td>256×256</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>with classifier-free guidance</td>
    </tr>
    <tr>
        <td>Unknown</td>
        <td>Midjourney v5</td>
        <td>1024×1024</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>one of the most renowned commercial software programs, known for its exceptional image generation performance.</td>
    </tr>
    <tr>
        <td >Diffusion+Decoder</td>
        <td>Stable Diffusion v1.4</td>
        <td>512×512</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>pretrained from the Stable Diffusion V1.2 checkpoint and fine-tuned on 225k steps at resolution 512x512 on Laion-Aesthetics V2 5+ dataset and drops 10% of the text-conditioning</td>
    </tr>
    <tr>
        <td >Diffusion+Decoder</td>
        <td>Stable Diffusion v1.5</td>
        <td>512×512</td>
        <td>16.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>training settings is the same as V1.4; is fine-tuned on 595,000 steps</td>
    </tr>
    <tr>
        <td>Diffusion</td>
        <td>VQDM</td>
        <td>256×256</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td></td>
    </tr>
    <tr>
        <td>Unknown</td>
        <td>wukong</td>
        <td>512×512</td>
        <td>12.0k</td>
        <td >ImageNet(input sentences follow the template "photo of class", with "class" being substituted by ImageNet labels.)</td>
        <td>multi</td>
        <td>trained on the largest Chinese open-source multimodal dataset</td>
    </tr>
    <tr>
        <td>Diffusion U-net</td>
        <td>DALL·E2</td>
        <td>256×256</td>
        <td>2k</td>
        <td></td>
        <td>multi</td>
        <td>generating images with COCO text</td>
    </tr>
    <tr>
        <td>Diffusion U-net</td>
        <td>DDPM</td>
        <td>256×256</td>
        <td>6k</td>
        <td>LSUN/celebahq</td>
        <td>3</td>
        <td>null</td>
    </tr>
</table> `)])]),tab2:r(({value:s,isActive:d})=>[e("div",U,[t(i,n(g({title:"CNN-generated images are surprisingly easy to spot... for now <b>[CNNSpot]</b>",desc:"<p>Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A. Efros.</p> <p><em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,</em>  (<b>CVPR</b>). 2020.</p> <hr /><p>CNNSpot proposes a simple yet effective fake image detector. They adopt ResNet-50 as a classifier and observe that data augmentation, including JPEG compression and Gaussian blur, can boost the generalization of the detector, which means the detector can generalize well to unseen architectures, datasets, and training methods.</p>",logo:"/CNNSpot.png",link:"https://peterwang512.github.io/CNNDetection/",color:"rgba(172, 216, 255, 0.15)"})),null,16),t(i,n(g({title:"Global Texture Enhancement for Fake Face Detection In the Wild <b>[GramNet]</b>",desc:"<p>Zhengzhe Liu, Xiaojuan Qi, and Philip H. S. Torr.</p> <p><em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,</em>  (<b>CVPR</b>). 2020.</p> <hr /><p>GramNet abserves the difference between the texture of fake faces and real ones. Motivated by the observation, they aim to improve the generalization and robustness of the detector by incorporating a global texture extraction into the common ResNet structure.</p>",logo:"/Gram.png",link:"https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Global_Texture_Enhancement_for_Fake_Face_Detection_in_the_Wild_CVPR_2020_paper.pdf",color:"rgba(253, 230, 138, 0.15)"})),null,16),t(i,n(g({title:"Leveraging Frequency Analysis for Deep Fake Image Recognition <b>[FreDect]</b>",desc:"<p>Joel Frank, Thorsten Eisenhofer, Lea Sch ̈onherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz.</p> <p><em>International conference on machine learning,</em> (<b>PMLR</b>). 2020.</p> <hr /><p>FreDect reveals that in frequency space, GAN-generated images exhibit severe artifacts that can be easily identified. Based on this analysis, they propose the frequency abnormality of fake images and conducts fake image detection from the frequency domain.</p>",logo:"/FreDect.png",link:"https://proceedings.mlr.press/v119/frank20a/frank20a.pdf",color:"rgba(172, 216, 255, 0.15)"})),null,16),t(i,n(g({title:"Fusing global and local features for generalized ai-synthesized image detection <b>[Fusing]</b>",desc:"<p>Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki Nagano‡, and Siwei Lyu.</p> <p><em>2022 IEEE International Conference on Image Processing, </em> (<b>ICIP</b>). 2022.</p> <hr /><p>Fusing uses a two-branch model to extract global spatial information from the whole image and local informative features from multiple patches selected by a novel patch selection module. Global and local features are fused by the Multi-head attention mechanism. Then, a classifier is trained to detect fake image based on the fused feature.</p>",logo:"/Fusing.png",link:"https://arxiv.org/pdf/2203.13964.pdf",color:"rgba(253, 230, 138, 0.15)"})),null,16),t(i,n(g({title:"Detecting Generated Images by Real Images <b>[LNP]</b>",desc:"<p>Bo Liu, Fan Yang, Xiuli Bi, Bin Xiao, Weisheng Li, and Xinbo Gao.</p> <p><em>European Conference on Computer Vision,</em> (<b>ECCV</b>). 2022.</p> <hr /><p>LNP observed that the noise pattern of real images exhibits similar characteristics in the frequency domain, while the generated images are far different. Therefore, it extracts the noise pattern of spatial images based on a well-trained denoising model. Then, it identifies fake images from the frequency domain of the noise pattern.</p>",logo:"/LNP.png",link:"https://dl.acm.org/doi/abs/10.1007/978-3-031-19781-9_6",color:"rgba(172, 216, 255, 0.15)"})),null,16),t(i,n(g({title:"Learning on Gradients： Generalized Artifacts Representation for GAN-Generated Images Detection <b>[LGrad]</b>",desc:"<p>Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, and Yunchao Wei.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em>  (<b>CVPR</b>). 2023.</p> <hr /><p>LGrad extracts gradient map, which is obtained by a well-trained image classifier, as the fingerprint of an GAN-generated image. This approach turns the data-dependent problem into a transformationmodel-dependent problem. Then, it conducts a binary classification task based on gradient maps.</p>",logo:"/Grad.png",link:"http://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Learning_on_Gradients_Generalized_Artifacts_Representation_for_GAN-Generated_Images_Detection_CVPR_2023_paper.pdf",color:"rgba(253, 230, 138, 0.15)"})),null,16),t(i,n(g({title:"Towards Universal Fake Image Detectors that Generalize Across Generative Models <b>[UnivFD]</b>",desc:"<p>Utkarsh Ojha, Yuheng Li, and Yong Jae Lee.</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em> (<b>CVPR</b>).2023.</p> <hr /><p>UnivFD uses a feature space extracted by a large pre-trained vision-language model (CLIP:ViT-L/14) to train the detector. The large pre-trained model leads to a smooth decision boundary, which improves the generalization of the detector</p>",logo:"/UnivFD.png",link:"https://openaccess.thecvf.com/content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf",color:"rgba(172, 216, 255, 0.15)"})),null,16),t(i,n(g({title:"DIRE for Diffusion-Generated Image Detection <b>[DIRE]</b>",desc:"<p>Zhendong Wang, Jianmin Bao, Wengang Zhou,Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li.</p> <p><em>International Conference on Computer Vision,</em> (<b>ICCV</b>).  2023.</p> <hr /><p>DIRE is dedicated to identifying fake images generated by diffusion-based images. They observe that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. Based on this observation, they leverage the error between an input image and its reconstruction counterpart by a pre-trained diffusion model as a fingerprint.</p>",logo:"/DIRE.png",link:"https://arxiv.org/pdf/2303.09295",color:"rgba(253, 230, 138, 0.15)"})),null,16),t(i,n(g({title:'<font color="red">PatchCraft：Exploring Texture Patch for Efficient AI-generated Image Detection</font> <b>[PatchCraft (RPTC)]</b>',desc:"<p>Nan Zhong, Yiran Xu, Zhenxing Qian, and Xinpeng Zhang.</p><hr /><p>PatchCraft leverages the inter-pixel correlation contrast between rich and poor texture regions within an image. Pixels in rich texture regions exhibit more significant fluctuations than those in poor texture regions. Based on this principle, we divide an image into multiple patches and reconstruct them into two images, comprising rich-texture and poor-texture patches respectively. Subsequently, we extract the inter-pixel correlation discrepancy feature between rich and poor texture regions. This feature serves as a universal fingerprint used for AI-generated image forensics across different generative models. </p>",logo:"/RPTC.png",link:"https://arxiv.org/abs/2311.12397",color:"rgba(172, 216, 255, 0.15)"})),null,16)])]),_:1}),p(` <table id="detection" class="mytable2">
        <tr>
            <th>Detection Method</th>
            <th>target model</th>
            <th>training set</th>
            <th>brief</th>
        </tr>
        <tr>
            <td>CNNSpot</td>
            <td>GAN</td>
            <td>progan</td>
            <td>train a ResNet50 classifier</td>
        </tr>
        <tr>
            <td>DCTAnalysis</td>
            <td>GAN</td>
            <td>progan</td>
            <td>perform a ridge regression on real and generated images, after applying a DCT</td>
        </tr>
        <tr>
            <td>PSM</td>
            <td>GAN</td>
            <td>progan</td>
            <td>combine global features from the whole image with the local features from informative patches</td>
        </tr>
        <tr>
            <td>Gram-Net</td>
            <td>GAN</td>
            <td>progan</td>
            <td>train a Gram-Net classifier</td>
        </tr>
        <tr>
            <td>LGrad</td>
            <td>GAN</td>
            <td>progan</td>
            <td>transform the images to the gradients by a pretrained CNN model, then train a ResNet50 classifier</td>
        </tr>
        <tr>
            <td>LNP-based</td>
            <td>GAN/Glow</td>
            <td>progan</td>
            <td>use the LNP(Learned Noise Patterns) to detect fake images with a ResNet50 classifier</td>
        </tr>
        <tr>
            <td>DIRE(lsun_iddpm)</td>
            <td>Diffusion</td>
            <td>ADM</td>
            <td>calculate the differences between origin image and its recovered version by a pretrained diffusion model</td>
        </tr>
        <tr>
            <td>DIRE(progan)</td>
            <td>Diffusion</td>
            <td>progan</td>
            <td></td>
        </tr>
        <tr>
            <td>RPTC</td>
            <td>All</td>
            <td>progan</td>
            <td></td>
        </tr>
</table> `)])}const W=h(S,[["render",B],["__file","Document.html.vue"]]);export{W as default};
